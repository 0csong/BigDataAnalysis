{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a5ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a499df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c103a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "\n",
    "print(\"Type of Response: \", type(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fe69af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4234984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print (type(wc), type(wc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea9aff6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Competition': 'World Cup',\n",
       " 'Year': 1930,\n",
       " 'Team': 'Argentina',\n",
       " 'Number': '',\n",
       " 'Position': 'GK',\n",
       " 'FullName': 'Ãngel Bossio',\n",
       " 'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
       " 'ClubCountry': 'Argentina',\n",
       " 'DateOfBirth': '1905-5-5',\n",
       " 'IsCaptain': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa0692fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_wcDf=spark.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc96ab",
   "metadata": {},
   "source": [
    "별표 1개 *: 리스트에서 인자 풀어내기\n",
    "\n",
    "예를 들어, range() 함수는 시작과 끝을 인자로 가지는 함수이다. 시작과 끝이라는 2개의 인자를 함수에 넘겨주어 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbf44753",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3dbe367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80a086de",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-57f8b854cfa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "list(range(myList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "336ba717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(*myList))\n",
    "#시작, 끝 변수가 따로 없을 경우, 리스트를 넘겨주고 여기서 \n",
    "#풀어서 하나씩 인자를 사용할 경우 *를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64929503",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "f() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-4972646694d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"~\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: f() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "def f(args):\n",
    "    for i in args:\n",
    "        print(i, end=\"~\")\n",
    "\n",
    "f(0, 1, 2, 3)\n",
    "#복수의 인자를 받을경우 *사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e9a7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0~1~2~3~"
     ]
    }
   ],
   "source": [
    "\n",
    "def f(*args):\n",
    "    for i in args:\n",
    "        print(i, end=\"~\")\n",
    "\n",
    "f(0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecfbe82",
   "metadata": {},
   "source": [
    "### 별표 2 개 **: dictionary에서 인자 풀어내기\n",
    "dictionary를 넘겨주고 여기서 key, value를 하나씩 사용할 경우 **를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "694dfc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsl in 2020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def printCapital(name, year):\n",
    "    print(f\"{name} in {year}\")\n",
    "\n",
    "myDict = {\"name\": \"jsl\", \"year\": 2020}\n",
    "printCapital(**myDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab09717",
   "metadata": {},
   "source": [
    "dictionary인자를 풀어서 Row에 넘겨주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f9c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "wcDf = spark.createDataFrame(Row(**x) for x in wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4b89a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aeb4a6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6695a2",
   "metadata": {},
   "source": [
    "## RDD생성\n",
    "RDD를 통해 DF생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eace3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcRdd=spark.sparkContext.parallelize(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "683caa07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Competition': 'World Cup',\n",
       "  'Year': 1930,\n",
       "  'Team': 'Argentina',\n",
       "  'Number': '',\n",
       "  'Position': 'GK',\n",
       "  'FullName': 'Ãngel Bossio',\n",
       "  'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
       "  'ClubCountry': 'Argentina',\n",
       "  'DateOfBirth': '1905-5-5',\n",
       "  'IsCaptain': False}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "412770c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDfFromRdd = spark.createDataFrame(wcRdd)\n",
    "wcDfFromRdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5f157",
   "metadata": {},
   "source": [
    "## 결측 값\n",
    "* null은 결측, 즉 \"no value\" 또는 \"nothing\"을 말한다.\n",
    "* NaN은 \"Not a Number\", 즉 수학에서 0.0/0.0과 같이 의미가 없는 연산의 결과를 말한다.\n",
    "\n",
    "IsCaptain 항목은 boolean 타입이라 isnan() 함수가 오류를 발생한다. 'isnan(`IsCaptain`)' due to data type mismatch: argument 1 requires (double or float) type, however, '`IsCaptain`' is of boolean type. 그래서 항목에서 제거하고 확인하기로 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "860956a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = wcDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "677287e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('IsCaptain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19dfc743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "|Competition|Year|Team|Number|Position|FullName|Club|ClubCountry|DateOfBirth|\n",
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "|          0|   0|   0|     0|       0|       0|   0|          0|          0|\n",
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "wcDf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a9ab9",
   "metadata": {},
   "source": [
    "## 형변환\n",
    "스키마를 자동으로 유추한 형식을 보면, DateOfBirth, Number 컬럼이 문자열로 인식되어 있어 만족스럽지 못하다. 컬럼 'DateOfBirth'는 'DoB'로 DateType(), 컬럼 'Number'는 'NumberInt'로 \"Integer\" 형으로 설정해보자.\n",
    "\n",
    "* DateType 형변환\n",
    "\n",
    "Python datetime을 사용한 DateType 컬럼 생성\n",
    "Python datetime은 날짜 문자열을 다음과 같은 년, 월, 일 형식으로 변환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd77db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print (datetime.strptime(\"11/25/1991\", '%m/%d/%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ef0d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n",
    "toDate = udf(lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e40a76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf = wcDf.withColumn('date1', toDate(wcDf['DateOfBirth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dcb0508a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 450, in mapper\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 450, in <genexpr>\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 83, in <lambda>\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-47-96a4807fab93>\", line 3, in <lambda>\n  File \"C:\\Anaconda3\\lib\\_strptime.py\", line 568, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"C:\\Anaconda3\\lib\\_strptime.py\", line 349, in _strptime\n    raise ValueError(\"time data %r does not match format %r\" %\nValueError: time data '1905-5-5' does not match format '%m/%d/%Y'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-43ea7a1ed5cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwcDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \"\"\"\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 450, in mapper\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 450, in <genexpr>\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 83, in <lambda>\n  File \"C:\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-47-96a4807fab93>\", line 3, in <lambda>\n  File \"C:\\Anaconda3\\lib\\_strptime.py\", line 568, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"C:\\Anaconda3\\lib\\_strptime.py\", line 349, in _strptime\n    raise ValueError(\"time data %r does not match format %r\" %\nValueError: time data '1905-5-5' does not match format '%m/%d/%Y'\n"
     ]
    }
   ],
   "source": [
    "wcDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c383a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf = wcDf.drop('date1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea0dd634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "_wcDfCasted=wcDf.withColumn('date2', to_date(wcDf['DateOfBirth'], 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "722e527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "942977c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "wcDfCasted = _wcDfCasted.withColumn('date3', _wcDfCasted['DateOfBirth'].cast(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe3e29ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40177b7",
   "metadata": {},
   "source": [
    "\n",
    "그대로 출력하면 오류가 발생할 수 있다. SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0라는 오류가 발생하면 spark.sql.legacy.timeParserPolicy=LEGACY라고 설정을 변경해주어야 한다.\n",
    "\n",
    "아래와 같이 to_data() 또는 cast() 함수를 사용하여 일자 형변환 결과를 올바르게 출력하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1fb43f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False, date2=datetime.date(1905, 5, 5), date3=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "wcDfCasted.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9266fa",
   "metadata": {},
   "source": [
    "앞서 사용했던 _myDf를 parquet형식으로 저장해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9260ca56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False, date3=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "import pyspark\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "# read url json\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()\n",
    "\n",
    "# read dictionary into Row\n",
    "wcDf = spark.createDataFrame(Row(**x) for x in wc)\n",
    "\n",
    "# cast DoB string into date, Number string into integer\n",
    "wcDfCasted = wcDf.withColumn('date3', wcDf['DateOfBirth'].cast(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))\n",
    "\n",
    "wcDfCasted.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f75b3",
   "metadata": {},
   "source": [
    "## S.5 DataFrame API 사용해 보기\n",
    "빈 DataFrame 생성\n",
    "\n",
    "비어있는 DataFrame을 생성하려면, 빈 schema를 설정하고 emptyRdd()를 사용해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e3571ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0, 10, 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4257fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d56c6a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c721fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = tDf.withColumn(\"id\", tDf._c0.cast(\"integer\"))\n",
    "tDf = tDf.withColumn(\"height\", tDf['_c1'].cast(\"double\"))\n",
    "tDf = tDf.withColumn(\"weight\", tDf['_c2'].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3840c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2db524d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = tDf.drop('_c0').drop('_c1').drop('_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9a0a84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "692d1e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, height=65.78, weight=112.99)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef482d",
   "metadata": {},
   "source": [
    "\n",
    "## 사용자정의 함수 udf\n",
    "UDF User Defined Functions는 사용자 정의함수로서 DataFrame의 withColumn() 함수와 같이 사용되어 새로운 컬럼을만드는 경우 유용하다. 보통 함수와 같이 함수명과 반환 값을 미리 정의해서 lambda 함수 또는 다른 함수를 사용할 수 있다. 다른 함수를 직접 사용할 수 없고, udf()를 통해서 호출해야 한다. udf()는 코드가 복잡한 경우에 함수를 분리해서 처리하면 유용하겠다.\n",
    "\n",
    "앞서 Pandas로 저장한 csv파일을 읽어서 DataFrame을 만들어 보자. header를 있는 그대로 가져오고, schema도 현재 데이터에서 추정하도록 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42391c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = spark\\\n",
    "        .read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferschema='true')\\\n",
    "        .load(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24e6d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "89ae02b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+\n",
      "|_c0|year|   name|height|\n",
      "+---+----+-------+------+\n",
      "|  0|   1|kim, js|   170|\n",
      "|  1|   1|lee, sm|   175|\n",
      "|  2|   2|lim, yg|   180|\n",
      "|  3|   2|    lee|   170|\n",
      "+---+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "34148c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uppercase(s):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c11a7dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "uppercase(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad4731a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-85e35a83822c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nameUpper\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muppercase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-76-cc3ea1335364>\u001b[0m in \u001b[0;36muppercase\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0muppercase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf = myDf.withColumn(\"nameUpper\", uppercase(myDf.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e8a8e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12d8bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myDf = myDf.withColumn(\"nameUpper\", upperUdf(myDf['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98f43263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+\n",
      "|_c0|year|   name|height|nameUpper|\n",
      "+---+----+-------+------+---------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|\n",
      "|  3|   2|    lee|   170|      LEE|\n",
      "+---+----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc090f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37506304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc63dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
