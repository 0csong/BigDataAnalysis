{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4efab3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_rdd_wordCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_rdd_wordCount.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "def doIt():\n",
    "    wikiRdd=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "    wc=wikiRdd\\\n",
    "        .flatMap(lambda x: x.split())\\\n",
    "        .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "        .reduceByKey(lambda x,y:x+y)\\\n",
    "        .sortByKey()\\\n",
    "        .collect()\n",
    "    for w in wc:\n",
    "        print (w)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Anaconda3\\\\python.exe\"\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\\\Anaconda3\\\\python.exe\"\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d9a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Anaconda3\\\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\\\Anaconda3\\\\python.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb71494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/Anaconda3/Lib/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('amplab', 1)\n",
      "('an', 2)\n",
      "('and', 1)\n",
      "('apache', 6)\n",
      "('at', 1)\n",
      "(\"berkeley's\", 1)\n",
      "('california', 1)\n",
      "('cluster', 1)\n",
      "('clusters', 1)\n",
      "('codebase', 1)\n",
      "('computing', 1)\n",
      "('data', 1)\n",
      "('developed', 1)\n",
      "('donated', 1)\n",
      "('entire', 1)\n",
      "('fault-tolerance', 1)\n",
      "('for', 1)\n",
      "('foundation', 1)\n",
      "('framework', 1)\n",
      "('has', 1)\n",
      "('implicit', 1)\n",
      "('interface', 1)\n",
      "('is', 1)\n",
      "('it', 1)\n",
      "('later', 1)\n",
      "('maintained', 1)\n",
      "('of', 1)\n",
      "('open', 1)\n",
      "('originally', 1)\n",
      "('parallelism', 1)\n",
      "('programming', 1)\n",
      "('provides', 1)\n",
      "('since', 1)\n",
      "('software', 1)\n",
      "('source', 1)\n",
      "('spark', 7)\n",
      "('the', 3)\n",
      "('to', 1)\n",
      "('university', 1)\n",
      "('was', 1)\n",
      "('which', 1)\n",
      "('wikipedia', 1)\n",
      "('with', 1)\n",
      "('소스', 1)\n",
      "('스파크', 4)\n",
      "('스파크는', 1)\n",
      "('아파치', 5)\n",
      "('오픈', 1)\n",
      "('컴퓨팅', 1)\n",
      "('클러스터', 1)\n",
      "('프레임워크이다', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/24 22:47:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/09/24 22:47:17 INFO SparkContext: Running Spark version 3.1.2\n",
      "21/09/24 22:47:17 INFO ResourceUtils: ==============================================================\n",
      "21/09/24 22:47:17 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/09/24 22:47:17 INFO ResourceUtils: ==============================================================\n",
      "21/09/24 22:47:17 INFO SparkContext: Submitted application: myApp\n",
      "21/09/24 22:47:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/09/24 22:47:17 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/09/24 22:47:17 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/09/24 22:47:17 INFO SecurityManager: Changing view acls to: nicky\n",
      "21/09/24 22:47:17 INFO SecurityManager: Changing modify acls to: nicky\n",
      "21/09/24 22:47:17 INFO SecurityManager: Changing view acls groups to: \n",
      "21/09/24 22:47:17 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/09/24 22:47:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nicky); groups with view permissions: Set(); users  with modify permissions: Set(nicky); groups with modify permissions: Set()\n",
      "21/09/24 22:47:19 INFO Utils: Successfully started service 'sparkDriver' on port 1059.\n",
      "21/09/24 22:47:19 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/09/24 22:47:19 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/09/24 22:47:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/09/24 22:47:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/09/24 22:47:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/09/24 22:47:19 INFO DiskBlockManager: Created local directory at C:\\Users\\nicky\\AppData\\Local\\Temp\\blockmgr-d87661b4-ed58-4a5f-a9b1-6a6efcff7efb\n",
      "21/09/24 22:47:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "21/09/24 22:47:19 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/09/24 22:47:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "21/09/24 22:47:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://LAPTOP-LKQ849JQ:4040\n",
      "21/09/24 22:47:19 INFO Executor: Starting executor ID driver on host LAPTOP-LKQ849JQ\n",
      "21/09/24 22:47:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1074.\n",
      "21/09/24 22:47:19 INFO NettyBlockTransferService: Server created on LAPTOP-LKQ849JQ:1074\n",
      "21/09/24 22:47:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/09/24 22:47:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-LKQ849JQ, 1074, None)\n",
      "21/09/24 22:47:19 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-LKQ849JQ:1074 with 434.4 MiB RAM, BlockManagerId(driver, LAPTOP-LKQ849JQ, 1074, None)\n",
      "21/09/24 22:47:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-LKQ849JQ, 1074, None)\n",
      "21/09/24 22:47:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-LKQ849JQ, 1074, None)\n",
      "21/09/24 22:47:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/nicky/Desktop/3-2학기/bigdata/spark-warehouse').\n",
      "21/09/24 22:47:20 INFO SharedState: Warehouse path is 'file:/C:/Users/nicky/Desktop/3-2학기/bigdata/spark-warehouse'.\n",
      "21/09/24 22:47:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 176.2 KiB, free 434.2 MiB)\n",
      "21/09/24 22:47:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)\n",
      "21/09/24 22:47:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on LAPTOP-LKQ849JQ:1074 (size: 27.2 KiB, free: 434.4 MiB)\n",
      "21/09/24 22:47:22 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "21/09/24 22:47:22 INFO FileInputFormat: Total input files to process : 1\n",
      "21/09/24 22:47:22 INFO SparkContext: Starting job: collect at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9\n",
      "21/09/24 22:47:22 INFO DAGScheduler: Registering RDD 3 (reduceByKey at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9) as input to shuffle 0\n",
      "21/09/24 22:47:22 INFO DAGScheduler: Got job 0 (collect at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9) with 1 output partitions\n",
      "21/09/24 22:47:22 INFO DAGScheduler: Final stage: ResultStage 1 (collect at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9)\n",
      "21/09/24 22:47:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "21/09/24 22:47:22 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "21/09/24 22:47:22 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9), which has no missing parents\n",
      "21/09/24 22:47:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)\n",
      "21/09/24 22:47:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 434.2 MiB)\n",
      "21/09/24 22:47:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on LAPTOP-LKQ849JQ:1074 (size: 7.3 KiB, free: 434.4 MiB)\n",
      "21/09/24 22:47:22 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388\n",
      "21/09/24 22:47:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9) (first 15 tasks are for partitions Vector(0))\n",
      "21/09/24 22:47:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "21/09/24 22:47:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (LAPTOP-LKQ849JQ, executor driver, partition 0, PROCESS_LOCAL, 4527 bytes) taskResourceAssignments Map()\n",
      "21/09/24 22:47:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "21/09/24 22:47:22 INFO HadoopRDD: Input split: file:/C:/Users/nicky/Desktop/3-2학기/bigdata/data/ds_spark_wiki.txt:0+583\n",
      "21/09/24 22:47:23 INFO PythonRunner: Times: total = 755, boot = 745, init = 9, finish = 1\n",
      "21/09/24 22:47:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1614 bytes result sent to driver\n",
      "21/09/24 22:47:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1180 ms on LAPTOP-LKQ849JQ (executor driver) (1/1)\n",
      "21/09/24 22:47:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/09/24 22:47:23 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 1075\n",
      "21/09/24 22:47:23 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9) finished in 1.305 s\n",
      "21/09/24 22:47:23 INFO DAGScheduler: looking for newly runnable stages\n",
      "21/09/24 22:47:23 INFO DAGScheduler: running: Set()\n",
      "21/09/24 22:47:23 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "21/09/24 22:47:23 INFO DAGScheduler: failed: Set()\n",
      "21/09/24 22:47:23 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9), which has no missing parents\n",
      "21/09/24 22:47:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.4 KiB, free 434.2 MiB)\n",
      "21/09/24 22:47:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.2 MiB)\n",
      "21/09/24 22:47:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on LAPTOP-LKQ849JQ:1074 (size: 5.5 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/24 22:47:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1388\n",
      "21/09/24 22:47:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at collect at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9) (first 15 tasks are for partitions Vector(0))\n",
      "21/09/24 22:47:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "21/09/24 22:47:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (LAPTOP-LKQ849JQ, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "21/09/24 22:47:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "21/09/24 22:47:23 INFO ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\n",
      "21/09/24 22:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
      "21/09/24 22:47:24 INFO PythonRunner: Times: total = 731, boot = 702, init = 29, finish = 0\n",
      "21/09/24 22:47:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2463 bytes result sent to driver\n",
      "21/09/24 22:47:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 803 ms on LAPTOP-LKQ849JQ (executor driver) (1/1)\n",
      "21/09/24 22:47:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21/09/24 22:47:24 INFO DAGScheduler: ResultStage 1 (collect at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9) finished in 0.816 s\n",
      "21/09/24 22:47:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/09/24 22:47:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "21/09/24 22:47:24 INFO DAGScheduler: Job 0 finished: collect at C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds_rdd_wordCount.py:9, took 2.207283 s\n",
      "21/09/24 22:47:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on LAPTOP-LKQ849JQ:1074 in memory (size: 7.3 KiB, free: 434.4 MiB)\n",
      "21/09/24 22:47:24 INFO SparkUI: Stopped Spark web UI at http://LAPTOP-LKQ849JQ:4040\n",
      "21/09/24 22:47:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/09/24 22:47:24 INFO MemoryStore: MemoryStore cleared\n",
      "21/09/24 22:47:24 INFO BlockManager: BlockManager stopped\n",
      "21/09/24 22:47:24 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/09/24 22:47:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/09/24 22:47:24 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/09/24 22:47:25 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/09/24 22:47:25 INFO ShutdownHookManager: Deleting directory C:\\Users\\nicky\\AppData\\Local\\Temp\\spark-f97eee85-5b84-4d7a-bd0a-789c717e1ad1\n",
      "21/09/24 22:47:25 INFO ShutdownHookManager: Deleting directory C:\\Users\\nicky\\AppData\\Local\\Temp\\spark-f97eee85-5b84-4d7a-bd0a-789c717e1ad1\\pyspark-7ec3e0d7-1246-4b8f-b2da-09f774be363f\n",
      "21/09/24 22:47:25 INFO ShutdownHookManager: Deleting directory C:\\Users\\nicky\\AppData\\Local\\Temp\\spark-0aa206bd-1562-45c5-84f6-6640c6e83d42\n"
     ]
    }
   ],
   "source": [
    "!spark-submit src/ds_rdd_wordCount.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
