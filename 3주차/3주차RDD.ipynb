{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1d6da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicky\\Desktop\\3-2학기\\bigdata\n",
      "C:\\Anaconda3\\python38.zip\n",
      "C:\\Anaconda3\\DLLs\n",
      "C:\\Anaconda3\\lib\n",
      "C:\\Anaconda3\n",
      "\n",
      "C:\\Anaconda3\\lib\\site-packages\n",
      "C:\\Anaconda3\\lib\\site-packages\\locket-0.2.1-py3.8.egg\n",
      "C:\\Anaconda3\\lib\\site-packages\\win32\n",
      "C:\\Anaconda3\\lib\\site-packages\\win32\\lib\n",
      "C:\\Anaconda3\\lib\\site-packages\\Pythonwin\n",
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\nicky\\.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for i in sys.path:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c669e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\python.exe\n",
      "C:\\Users\\nicky\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\n",
      "C:\\Users\\nicky\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python\n",
    "#!where pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97412a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Anaconda3\\\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\\\Anaconda3\\\\python.exe\"\n",
    "#설정->환경변수->시스템 변수->새로만들기->PYSPARK_PYTHON과 \n",
    "#where python결과를 넣어줌, PYSPARK_DRIVER_PYTHON도 마찬가지\n",
    "#중요한건 기본경로에는 \\가 한개이지만 window에서는 환경변수떄 \\\\로 넣어줘야함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48d5a0",
   "metadata": {},
   "source": [
    "## SparkSession 생성\n",
    "* Spark를 사용하려면 SparkSession 객체를 생성해야 한다.\n",
    "\n",
    "SparkSession을 생성해 보자. SparkSesion은 sql 모듈로 'pyspark.sql.SparkSession'을 클라이언트로 사용한다. 필요한 설정은 SparkSession이 만들지기 전에 해 두어야 한다. 여기서는 설정을 별도로 하지 않고 비워 놓았다. SparkSession은 builder.getOrCreate() 함수를 호출하여, 기존의 session 또는 새로 생성하여 사용한다. 함수 getOrCreate() 함수는 singleton 패턴으로 한 번에 하나의 세션만이 존재하도록 한다. SparkSession을 종료하려면 stop() 함수를 호출한다.\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca7290",
   "metadata": {},
   "source": [
    "Spark를 실행하기 전 필수적으로 설정해야 하는 항목은\n",
    "\n",
    "* master: (1) 분산의 경우 master URL 또는 (2) 로컬인 경우 local[]라고 적어준다. 즉 local의 수는 CPU core의 수를 의미한다. 예를 들어 local[*]는 가능한 최대한의 core를 사용한다는 의미이다. 예를 들어, local[5]라고 하면, core의 수가 2개라고 하더라도 데이터는 5개의 partitions로 나누어져 주어진다.\n",
    " * local은 Spark를 로컬에서 실행한다는 의미이다.\n",
    " * local[n]는 worker의 쓰레드를 n개로 한다는 의미. CPU core의 개수에 맞추어 설정하자.\n",
    " * local[*] 는 가능하면 가용한 모든 쓰레드를 사용한다는 의미 (Runtime.getRuntime.availableProcessors()로 그 수를 알 수 있다)\n",
    "* appName: 앱의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f5b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "#myConf=pyspark.SparkConf().set(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1c2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version \t: 3.1.2\n",
      "Spark App \t: myApp\n",
      "Spark Master \t: local\n",
      "Spark Host \t: LAPTOP-LKQ849JQ\n"
     ]
    }
   ],
   "source": [
    "print (\"Spark version \\t: {}\".format(spark.version))\n",
    "print (\"Spark App \\t: {}\".format(spark.conf.get('spark.app.name')))\n",
    "print (\"Spark Master \\t: {}\".format(spark.conf.get('spark.master')))\n",
    "print (\"Spark Host \\t: {}\".format(spark.conf.get('spark.driver.host')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec53f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/C:/Users/nicky/Desktop/3-2학기/bigdata/spark-warehouse'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e0e5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.name', 'myApp'),\n",
       " ('spark.app.startTime', '1632143928006'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', 'LAPTOP-LKQ849JQ'),\n",
       " ('spark.app.id', 'local-1632143930763'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '4130'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/C:/Users/nicky/Desktop/3-2학기/bigdata/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb6af10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.name', 'myApp'),\n",
       " ('spark.app.startTime', '1632143928006'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', 'LAPTOP-LKQ849JQ'),\n",
       " ('spark.app.id', 'local-1632143930763'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '4130'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/C:/Users/nicky/Desktop/3-2학기/bigdata/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80090f4",
   "metadata": {},
   "source": [
    " 설정의 변경\n",
    "\n",
    "SparkSession이 일단 만들어지고 나서는, sparkContext를 경유해서 \n",
    "\n",
    "spark.sparkContext._conf.set() 함수로 설정을 변경할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f1d2f",
   "metadata": {},
   "source": [
    "\n",
    "Spark를 실행하면서 라이브러리를 설정해야 할 필요도 생겨나게 된다. 다음은 mongo, graphframes, csv 등의 라이브러리가 설정되어 있는 내용을 보여주고 있다. 라이브러리는 https://spark-packages.org 를 방문해서 찾아서 사용하면 된다. 실제로 설치하지 않고, 명명규칙에 따라, maven에서 하는 것과 같이, 콜론으로 구분해, 패키지명과 라이브러리를 버전정보와 같이 적어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5220b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4195fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd1 = spark.sparkContext.parallelize(myList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233b0466",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b72efc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a4a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e2cbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wikipedia'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb0739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value='Wikipedia')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "print (myDf.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb5ad4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print (type(myDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2fbc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fa83eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f543aba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "myList=myRdd4.take(5)\n",
    "print (type(myList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8ad89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds3_popCsvRead.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile src/ds3_popCsvRead.py\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    print (\"---------RESULT-----------\")\n",
    "    popDf = spark\\\n",
    "                .read.option(\"charset\", \"euc-kr\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(os.path.join(\"data\",\"경기도 의정부시_인구현황_20200904.csv\"))\n",
    "    popDf.show(5)\n",
    "    agedDf = spark\\\n",
    "                .read.option(\"charset\", \"euc-kr\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(os.path.join(\"data\",\"제주특별자치도 서귀포시_고령화비율및노령화지수현황_20200623.csv\"))\n",
    "    agedDf.show(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff188f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------RESULT-----------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds3_popCsvRead.py\", line 28, in <module>\n",
      "    doIt()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/Anaconda3/Lib/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/10/20 18:57:32 INFO SparkContext: Running Spark version 3.1.2\n",
      "21/10/20 18:57:32 INFO ResourceUtils: ==============================================================\n",
      "21/10/20 18:57:32 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/10/20 18:57:32 INFO ResourceUtils: ==============================================================\n",
      "21/10/20 18:57:32 INFO SparkContext: Submitted application: myApp\n",
      "21/10/20 18:57:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/10/20 18:57:32 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/10/20 18:57:32 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/10/20 18:57:32 INFO SecurityManager: Changing view acls to: nicky\n",
      "21/10/20 18:57:32 INFO SecurityManager: Changing modify acls to: nicky\n",
      "21/10/20 18:57:32 INFO SecurityManager: Changing view acls groups to: \n",
      "21/10/20 18:57:32 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/10/20 18:57:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nicky); groups with view permissions: Set(); users  with modify permissions: Set(nicky); groups with modify permissions: Set()\n",
      "21/10/20 18:57:37 INFO Utils: Successfully started service 'sparkDriver' on port 4565.\n",
      "21/10/20 18:57:37 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/10/20 18:57:37 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/10/20 18:57:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/10/20 18:57:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/10/20 18:57:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/10/20 18:57:37 INFO DiskBlockManager: Created local directory at C:\\Users\\nicky\\AppData\\Local\\Temp\\blockmgr-3c0c6bb4-fe69-4bfd-839f-697aac10d8ac\n",
      "21/10/20 18:57:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "21/10/20 18:57:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/10/20 18:57:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "21/10/20 18:57:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://LAPTOP-LKQ849JQ:4040\n",
      "21/10/20 18:57:38 INFO Executor: Starting executor ID driver on host LAPTOP-LKQ849JQ\n",
      "21/10/20 18:57:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4585.\n",
      "21/10/20 18:57:38 INFO NettyBlockTransferService: Server created on LAPTOP-LKQ849JQ:4585\n",
      "21/10/20 18:57:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/10/20 18:57:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-LKQ849JQ, 4585, None)\n",
      "21/10/20 18:57:38 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-LKQ849JQ:4585 with 434.4 MiB RAM, BlockManagerId(driver, LAPTOP-LKQ849JQ, 4585, None)\n",
      "21/10/20 18:57:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-LKQ849JQ, 4585, None)\n",
      "21/10/20 18:57:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-LKQ849JQ, 4585, None)\n",
      "21/10/20 18:57:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/nicky/Desktop/3-2학기/bigdata/spark-warehouse').\n",
      "21/10/20 18:57:39 INFO SharedState: Warehouse path is 'file:/C:/Users/nicky/Desktop/3-2학기/bigdata/spark-warehouse'.\n",
      "21/10/20 18:57:43 INFO SparkUI: Stopped Spark web UI at http://LAPTOP-LKQ849JQ:4040\n",
      "21/10/20 18:57:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/10/20 18:57:43 INFO MemoryStore: MemoryStore cleared\n",
      "21/10/20 18:57:43 INFO BlockManager: BlockManager stopped\n",
      "21/10/20 18:57:43 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/10/20 18:57:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/10/20 18:57:43 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/10/20 18:57:43 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/10/20 18:57:43 INFO ShutdownHookManager: Deleting directory C:\\Users\\nicky\\AppData\\Local\\Temp\\spark-b467c878-65e1-4f82-ba02-dd06f2154155\n",
      "21/10/20 18:57:43 INFO ShutdownHookManager: Deleting directory C:\\Users\\nicky\\AppData\\Local\\Temp\\spark-b467c878-65e1-4f82-ba02-dd06f2154155\\pyspark-2ed88f96-0c29-424c-bd72-b615bdfc2b1f\n",
      "21/10/20 18:57:43 INFO ShutdownHookManager: Deleting directory C:\\Users\\nicky\\AppData\\Local\\Temp\\spark-c258502c-c413-46ad-8cea-4bbdb23d6bca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"C:/Users/nicky/Desktop/3-2학기/bigdata/src/ds3_popCsvRead.py\", line 8, in doIt\n",
      "    popDf = spark\\\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\readwriter.py\", line 737, in csv\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 1304, in __call__\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 117, in deco\n",
      "pyspark.sql.utils.AnalysisException: Path does not exist: file:/C:/Users/nicky/Desktop/3-2학기/bigdata/data/경기도 의정부시_인구현황_20200904.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!spark-submit src/ds3_popCsvRead.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
