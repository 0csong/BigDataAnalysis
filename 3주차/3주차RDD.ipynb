{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1d6da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicky\\Desktop\\3-2학기\\빅데이터분석\n",
      "C:\\Anaconda3\\python38.zip\n",
      "C:\\Anaconda3\\DLLs\n",
      "C:\\Anaconda3\\lib\n",
      "C:\\Anaconda3\n",
      "\n",
      "C:\\Anaconda3\\lib\\site-packages\n",
      "C:\\Anaconda3\\lib\\site-packages\\locket-0.2.1-py3.8.egg\n",
      "C:\\Anaconda3\\lib\\site-packages\\win32\n",
      "C:\\Anaconda3\\lib\\site-packages\\win32\\lib\n",
      "C:\\Anaconda3\\lib\\site-packages\\Pythonwin\n",
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\nicky\\.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for i in sys.path:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c669e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\python.exe\n",
      "C:\\Users\\nicky\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\n",
      "C:\\Users\\nicky\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python\n",
    "#!where pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97412a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"C:\\Anaconda3\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\Anaconda3\\python.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48d5a0",
   "metadata": {},
   "source": [
    "## SparkSession 생성\n",
    "* Spark를 사용하려면 SparkSession 객체를 생성해야 한다.\n",
    "\n",
    "SparkSession을 생성해 보자. SparkSesion은 sql 모듈로 'pyspark.sql.SparkSession'을 클라이언트로 사용한다. 필요한 설정은 SparkSession이 만들지기 전에 해 두어야 한다. 여기서는 설정을 별도로 하지 않고 비워 놓았다. SparkSession은 builder.getOrCreate() 함수를 호출하여, 기존의 session 또는 새로 생성하여 사용한다. 함수 getOrCreate() 함수는 singleton 패턴으로 한 번에 하나의 세션만이 존재하도록 한다. SparkSession을 종료하려면 stop() 함수를 호출한다.\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca7290",
   "metadata": {},
   "source": [
    "Spark를 실행하기 전 필수적으로 설정해야 하는 항목은\n",
    "\n",
    "* master: (1) 분산의 경우 master URL 또는 (2) 로컬인 경우 local[]라고 적어준다. 즉 local의 수는 CPU core의 수를 의미한다. 예를 들어 local[*]는 가능한 최대한의 core를 사용한다는 의미이다. 예를 들어, local[5]라고 하면, core의 수가 2개라고 하더라도 데이터는 5개의 partitions로 나누어져 주어진다.\n",
    " * local은 Spark를 로컬에서 실행한다는 의미이다.\n",
    " * local[n]는 worker의 쓰레드를 n개로 한다는 의미. CPU core의 개수에 맞추어 설정하자.\n",
    " * local[*] 는 가능하면 가용한 모든 쓰레드를 사용한다는 의미 (Runtime.getRuntime.availableProcessors()로 그 수를 알 수 있다)\n",
    "* appName: 앱의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f5b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "#myConf=pyspark.SparkConf().set(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1c2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version \t: 3.1.2\n",
      "Spark App \t: myApp\n",
      "Spark Master \t: local\n",
      "Spark Host \t: LAPTOP-LKQ849JQ\n"
     ]
    }
   ],
   "source": [
    "print (\"Spark version \\t: {}\".format(spark.version))\n",
    "print (\"Spark App \\t: {}\".format(spark.conf.get('spark.app.name')))\n",
    "print (\"Spark Master \\t: {}\".format(spark.conf.get('spark.master')))\n",
    "print (\"Spark Host \\t: {}\".format(spark.conf.get('spark.driver.host')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec53f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/C:/Users/nicky/Desktop/3-2학기/빅데이터분석/spark-warehouse'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e0e5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/C:/Users/nicky/Desktop/3-2학기/빅데이터분석/spark-warehouse'),\n",
       " ('spark.app.name', 'myApp'),\n",
       " ('spark.app.startTime', '1632022678320'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', 'LAPTOP-LKQ849JQ'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1632022680739'),\n",
       " ('spark.driver.port', '10649')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb6af10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/C:/Users/nicky/Desktop/3-2학기/빅데이터분석/spark-warehouse'),\n",
       " ('spark.app.name', 'myApp'),\n",
       " ('spark.app.startTime', '1632022678320'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', 'LAPTOP-LKQ849JQ'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1632022680739'),\n",
       " ('spark.driver.port', '10649')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80090f4",
   "metadata": {},
   "source": [
    " 설정의 변경\n",
    "\n",
    "SparkSession이 일단 만들어지고 나서는, sparkContext를 경유해서 \n",
    "\n",
    "spark.sparkContext._conf.set() 함수로 설정을 변경할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f1d2f",
   "metadata": {},
   "source": [
    "\n",
    "Spark를 실행하면서 라이브러리를 설정해야 할 필요도 생겨나게 된다. 다음은 mongo, graphframes, csv 등의 라이브러리가 설정되어 있는 내용을 보여주고 있다. 라이브러리는 https://spark-packages.org 를 방문해서 찾아서 사용하면 된다. 실제로 설치하지 않고, 명명규칙에 따라, maven에서 하는 것과 같이, 콜론으로 구분해, 패키지명과 라이브러리를 버전정보와 같이 적어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5220b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4195fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd1 = spark.sparkContext.parallelize(myList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233b0466",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72efc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e2cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "print (myDf.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ad4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(myDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fbc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa83eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=myRdd4.take(5)\n",
    "print (type(myList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ad89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
